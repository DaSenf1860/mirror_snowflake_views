{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be64a1-ab3b-45a3-9c58-1cf5aff103e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install snowflake-connector-python==3.14.0 --quiet\n",
    "# %pip install msfabricpysdkcore --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd438d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msfabricpysdkcore import FabricClientCore\n",
    "try:\n",
    "    import sempy.fabric as fabric\n",
    "    local = False\n",
    "except:\n",
    "    local = True\n",
    "    from azure.identity import DefaultAzureCredential, AzureCliCredential\n",
    "    local_credentials = AzureCliCredential()\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient, DataLakeDirectoryClient\n",
    "import os\n",
    "from time import sleep\n",
    "import pyodbc\n",
    "import struct\n",
    "from azure.identity import ClientSecretCredential\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace8318-e32f-4521-85b9-f41b16993311",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "sql_database_workspace_id = None #\"dafsd84f7\"\n",
    "mirrored_db_workspace_id = None #\"dafsd84f7\"\n",
    "\n",
    "mirrored_db_id = None #\"dafsd84f7\"\n",
    "\n",
    "# If you are running this notebook locally, please set the workspace ID to the one you are using in your Fabric workspace.\n",
    "# If you are running this notebook in Fabric, please leave the workspace ID as None.\n",
    "# The code will automatically get the workspace ID of the workspace you are using for this notebook.\n",
    "\n",
    "if sql_database_workspace_id is None:\n",
    "    sql_database_workspace_id = fabric.get_workspace_id()\n",
    "if mirrored_db_workspace_id is None:\n",
    "    mirrored_db_workspace_id = fabric.get_workspace_id()\n",
    "\n",
    "new_entries = [{\"workspace_id\": mirrored_db_workspace_id,\n",
    "                \"mirrored_db_id\": mirrored_db_id,\n",
    "                \"snowflake_db\": \"STREAMTEST\",\n",
    "                \"snowflake_schema\": \"STREAMTESTSCHEMA\",\n",
    "                \"view_name\": \"CUSTOMERS_PER_STOCKITEMKEY2\",\n",
    "                \"keyvault\": \"openmirroring\"}\n",
    "             ]   \n",
    "\n",
    "sql_database_name=\"metadatamirroring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcc = FabricClientCore()\n",
    "sql_db = fcc.get_sql_database(workspace_id=sql_database_workspace_id, sql_database_name=sql_database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cc290-fa37-49e5-8d64-6363fec17c95",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "server = sql_db.properties[\"serverFqdn\"][:-5]\n",
    "databasename = sql_db.properties[\"databaseName\"]\n",
    "database = \"{\" + f\"{databasename}\" +\"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b923c9-3edd-4d6f-a9d9-ee897c854126",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get Azure token using DefaultAzureCredential\n",
    "if local:\n",
    "    token_bytes = local_credentials.get_token(\"https://database.windows.net/.default\").token.encode(\"UTF-16-LE\")\n",
    "else:\n",
    "    token_bytes = notebookutils.credentials.getToken(\"https://database.windows.net/.default\").encode(\"UTF-16-LE\")\n",
    "token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
    "SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by microsoft in msodbcsql.h\n",
    "\n",
    "# Connection parameters\n",
    "connection_string = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={server};Database={database};\"\n",
    "\n",
    "for new_entry in new_entries:\n",
    "    workspace_id = new_entry[\"workspace_id\"]\n",
    "    mirrored_db_id = new_entry[\"mirrored_db_id\"]\n",
    "    snowflake_db = new_entry[\"snowflake_db\"]\n",
    "    snowflake_schema = new_entry[\"snowflake_schema\"]\n",
    "    view_name = new_entry[\"view_name\"]\n",
    "    identifiers = new_entry.get(\"identifiers\", None)\n",
    "    keyvault = new_entry[\"keyvault\"]\n",
    "\n",
    "    if identifiers is not None:\n",
    "        value_str = f\"'{workspace_id}', '{mirrored_db_id}', '{snowflake_db}', '{snowflake_schema}', '{view_name}', '{identifiers}', '{keyvault}'\"\n",
    "    else:\n",
    "        value_str = f\"'{workspace_id}', '{mirrored_db_id}', '{snowflake_db}', '{snowflake_schema}', '{view_name}', NULL, '{keyvault}'\"\n",
    "\n",
    "    query = f\"\"\"INSERT INTO [dbo].[Metadata] (\n",
    "        [workspace_id],\n",
    "        [mirrored_db_id],\n",
    "       [snowflake_db],\n",
    "        [snowflake_schema],\n",
    "        [view_name],\n",
    "        [identifiers],\n",
    "        [keyvault]\n",
    "    ) VALUES (\n",
    "        {value_str}\n",
    "    );\n",
    "    \"\"\"\n",
    "    conn = pyodbc.connect(connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(query + \"COMMIT;\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Connect with Entra ID (Azure AD) token\n",
    "conn = pyodbc.connect(connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Test the connection\n",
    "cursor.execute(\"SELECT * FROM Metadata WHERE STATUS = 'not initialized' ORDER BY ID DESC;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "column_names = [column[0] for column in cursor.description]\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "df = pd.DataFrame.from_records(rows, columns=column_names)\n",
    "\n",
    "if df[\"ID\"].any():\n",
    "    count = df[\"ID\"].count()\n",
    "    print(f\"Initializing {count} views\")\n",
    "    \n",
    "else:\n",
    "    print(\"Nothing to initialize\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea28df-3176-48a7-bf5d-f7ab27e768e7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def run_for_one_view(df):\n",
    "    streamable = False\n",
    "\n",
    "    if local:\n",
    "\n",
    "        # Get the credentials by calling the API of the Key Vault 'https://{df[\"keyvault\"]}.vault.azure.net/'\n",
    "        def get_secret(secret_name):\n",
    "            vaultBaseUrl = f\"https://{df['keyvault']}.vault.azure.net\"\n",
    "            \n",
    "            keyvault_token = local_credentials.get_token(vaultBaseUrl).token\n",
    "            \n",
    "            request_headers = {\n",
    "                \"Authorization\": f\"Bearer {keyvault_token}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            keyvault_url = f\"{vaultBaseUrl}/secrets/{secret_name}?api-version=7.4\"\n",
    "            response = requests.get(keyvault_url, headers=request_headers)\n",
    "            if response.status_code == 200:\n",
    "                secret_value = response.json()[\"value\"]\n",
    "                return secret_value\n",
    "            else:\n",
    "                raise Exception(f\"Failed to retrieve secret: {response.status_code} - {response.text}\")\n",
    "        mirroringspntenantid = get_secret('mirroringspntenantid')\n",
    "        mirroringspnclientid = get_secret('mirroringspnclientid')\n",
    "        mirroringspnclientsecret = get_secret('mirroringspnclientsecret')\n",
    "\n",
    "        snowflakeaccount = get_secret('snowflakeaccount')\n",
    "        snowflakeuser = get_secret('snowflakeuser')\n",
    "        snowflakepassword = get_secret('snowflakepassword')\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        mirroringspntenantid = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'mirroringspntenantid')\n",
    "        mirroringspnclientid = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'mirroringspnclientid')\n",
    "        mirroringspnclientsecret = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'mirroringspnclientsecret')\n",
    "\n",
    "        snowflakeaccount = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'snowflakeaccount')\n",
    "        snowflakeuser = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'snowflakeuser')\n",
    "        snowflakepassword = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'snowflakepassword')\n",
    "\n",
    "    credential = ClientSecretCredential(tenant_id=str(mirroringspntenantid),\n",
    "                        client_id=mirroringspnclientid,\n",
    "                        client_secret=mirroringspnclientsecret)\n",
    "    mirrored_db_workspace_id = df[\"workspace_id\"]\n",
    "    mirrored_db_id = df[\"mirrored_db_id\"]\n",
    "\n",
    "    snowflake_db = df[\"snowflake_db\"]\n",
    "    snowflake_schema = df[\"snowflake_schema\"]\n",
    "    view_name = df[\"view_name\"]\n",
    "    identifiers = df[\"identifiers\"]\n",
    "    identifiers = None if identifiers is None else json.loads(identifiers)\n",
    "    conn = snowflake.connector.connect(account=snowflakeaccount,\n",
    "                                    user=snowflakeuser,\n",
    "                                    password=snowflakepassword)\n",
    "    def run_query(query):\n",
    "        with conn.cursor() as cur:\n",
    "            results = cur.execute(query).fetchall()\n",
    "        return results\n",
    "\n",
    "   \n",
    "    if identifiers:\n",
    "        identifier_concat = \",\".join(identifiers)\n",
    "        query = f\"\"\"CREATE or REPLACE VIEW {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR AS SELECT *, CONCAT({identifier_concat},HASH(*)) as hash_ FROM {snowflake_db}.{snowflake_schema}.{view_name}\"\"\"\n",
    "    else:\n",
    "        query = f\"\"\"CREATE or REPLACE VIEW {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR AS SELECT *, TO_VARCHAR(HASH(*)) as hash_ FROM {snowflake_db}.{snowflake_schema}.{view_name}\"\"\"\n",
    "    run_query(query)\n",
    "    query = f\"\"\"CREATE or REPLACE TABLE {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_HASHES (HASH_ VARCHAR);\"\"\"\n",
    "    run_query(query)\n",
    "    query = f\"\"\"ALTER TABLE {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_HASHES ADD COLUMN id INT AUTOINCREMENT;\"\"\"\n",
    "    run_query(query)\n",
    "    query = f\"\"\"COMMIT;\"\"\"\n",
    "    run_query(query)\n",
    "    query = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_OLD as (\n",
    "\n",
    "    select \n",
    "    *\n",
    "    from \n",
    "    {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_HASHES t1\n",
    "    where \n",
    "    not exists (select \n",
    "                    1\n",
    "                from \n",
    "                    {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR t2\n",
    "                where\n",
    "                    t2.hash_ = t1.hash_\n",
    "                ))\n",
    "\n",
    "    \"\"\"\n",
    "    run_query(query)\n",
    "    query = f\"\"\"\n",
    "\n",
    "    CREATE OR REPLACE VIEW {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_NEW as (\n",
    "    select \n",
    "    *\n",
    "    from \n",
    "    {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR t1\n",
    "    where \n",
    "    not exists (select \n",
    "                    1\n",
    "                from \n",
    "                    {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_HASHES t2\n",
    "                where\n",
    "                    t2.hash_ = t1.hash_\n",
    "                ))\n",
    "    \"\"\"\n",
    "    run_query(query)\n",
    "\n",
    "    mirrored_db_path = f\"{mirrored_db_id}/Files/LandingZone\"\n",
    "\n",
    "    def get_service_client_token_credential() -> DataLakeServiceClient:\n",
    "        account_url = f\"https://onelake.dfs.fabric.microsoft.com/\"\n",
    "        token_credential = credential\n",
    "\n",
    "        service_client = DataLakeServiceClient(account_url, credential=token_credential)\n",
    "\n",
    "        return service_client\n",
    "\n",
    "\n",
    "    def list_directory_contents(file_system_client: FileSystemClient, directory_name: str):\n",
    "        paths = file_system_client.get_paths(path=directory_name)\n",
    "\n",
    "        return paths\n",
    "    # %%\n",
    "    def upload_file_to_directory(directory_client: DataLakeDirectoryClient, local_path: str, file_name: str):\n",
    "        file_client = directory_client.get_file_client(file_name)\n",
    "\n",
    "        with open(file=os.path.join(local_path, file_name), mode=\"rb\") as data:\n",
    "            file_client.upload_data(data, overwrite=True)\n",
    "\n",
    "    dlsc = get_service_client_token_credential()\n",
    "    fsc = dlsc.get_file_system_client(mirrored_db_workspace_id)\n",
    "\n",
    "    fsc.create_directory(mirrored_db_path)\n",
    "    print(f\"Directory {mirrored_db_path} created\")\n",
    "\n",
    "    mirrored_db_path = f\"{mirrored_db_id}/Files/LandingZone\"\n",
    "\n",
    "    contents = list_directory_contents(fsc, directory_name=mirrored_db_path)\n",
    "    content_names = [content[\"name\"] for content in contents]\n",
    "    table_path = mirrored_db_path + f\"/{view_name}\"\n",
    "    if table_path not in content_names:\n",
    "        fsc.create_directory(table_path)\n",
    "        print(f\"Directory {table_path} created\")\n",
    "\n",
    "    contents = list_directory_contents(fsc, directory_name=mirrored_db_path)\n",
    "    content_names = [content[\"name\"] for content in contents]\n",
    "    metadata_path = mirrored_db_path + f\"/{view_name}/_metadata.json\"\n",
    "    if metadata_path not in content_names:\n",
    "        import json, os\n",
    "        json_content = {\n",
    "                    \"keyColumns\": [\n",
    "                    \"METADATAROW_ID\"\n",
    "                    ],\n",
    "                    \"fileFormat\": \"parquet\"\n",
    "                }\n",
    "        os.makedirs(\"upload\", exist_ok=True)\n",
    "        json.dump(json_content, open(\"upload/_metadata.json\", \"w\"), indent=4)\n",
    "        dldc = fsc.get_directory_client(table_path)\n",
    "        upload_file_to_directory(dldc, local_path=\"upload\", file_name=\"_metadata.json\")\n",
    "        print(f\"File {metadata_path} uploaded\")\n",
    "\n",
    "    status = fcc.get_mirroring_status(workspace_id=mirrored_db_workspace_id, mirrored_database_id=mirrored_db_id)\n",
    "    if 'status' not in status or status['status'].upper() != 'RUNNING':\n",
    "        fcc.start_mirroring(workspace_id=mirrored_db_workspace_id, mirrored_database_id=mirrored_db_id)\n",
    "\n",
    "    for _ in range(20):\n",
    "        if 'status' not in status or status['status'].upper() != 'RUNNING':\n",
    "            status = fcc.get_mirroring_status(workspace_id=mirrored_db_workspace_id, mirrored_database_id=mirrored_db_id)\n",
    "            sleep(5)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if local:\n",
    "        token_bytes = local_credentials.get_token(\"https://database.windows.net/.default\").token.encode(\"UTF-16-LE\")\n",
    "    else:\n",
    "        token_bytes = notebookutils.credentials.getToken(\"https://database.windows.net/.default\").encode(\"UTF-16-LE\")\n",
    "    token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
    "\n",
    "    # Connect with Entra ID (Azure AD) token\n",
    "    conn = pyodbc.connect(connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Test the connection\n",
    "    id = df[\"ID\"]\n",
    "    query = f\"\"\"UPDATE Metadata \n",
    "                SET [status] = 'active',\n",
    "                    [streamable] = '{streamable}'\n",
    "                WHERE ID = {id}; COMMIT;\"\"\"\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Close the connection\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e683b76-e7c3-46f6-97b4-b3360c2bf3d0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    run_for_one_view(row)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "c494b827-ca90-4f17-a799-e00cc98581de",
    "workspaceId": "5bddda80-bbab-40cf-b106-abb535fae28c"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
