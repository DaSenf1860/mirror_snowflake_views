{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46a40f-3c65-4fb2-bc24-8fa918d9e7ff",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from msfabricpysdkcore import FabricClientCore\n",
    "try:\n",
    "    import sempy.fabric as fabric\n",
    "    local = False\n",
    "except:\n",
    "    local = True\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient, DataLakeDirectoryClient\n",
    "import os\n",
    "import pyodbc\n",
    "import struct\n",
    "from azure.identity import ClientSecretCredential\n",
    "import requests\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_database_name=\"metadatamirroring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aeacc2-cf83-40ae-9b20-0c303c96ddc1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "if local:\n",
    "    workspace_id = \"14110asdfasfa79577\"\n",
    "    local_credentials = DefaultAzureCredential()\n",
    "else:\n",
    "    workspace_id = fabric.get_workspace_id()\n",
    "fcc = FabricClientCore()\n",
    "sql_db = fcc.get_sql_database(workspace_id=workspace_id, sql_database_name=sql_database_name)\n",
    "\n",
    "\n",
    "server = sql_db.properties[\"serverFqdn\"][:-5]\n",
    "databasename = sql_db.properties[\"databaseName\"]\n",
    "database = \"{\" + f\"{databasename}\" +\"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70869995-9ad9-421c-a3a7-69c1b2d2f803",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a34b3-06de-42c5-b4d6-78a4069bfbf6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get Azure token using DefaultAzureCredential\n",
    "if local:\n",
    "    token_bytes = local_credentials.get_token(\"https://database.windows.net/.default\").token.encode(\"UTF-16-LE\")\n",
    "else:\n",
    "    token_bytes = notebookutils.credentials.getToken(\"https://database.windows.net/.default\").encode(\"UTF-16-LE\")\n",
    "token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
    "SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by microsoft in msodbcsql.h\n",
    "\n",
    "# Connection parameters\n",
    "connection_string = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={server};Database={database};\"\n",
    "\n",
    "# Connect with Entra ID (Azure AD) token\n",
    "conn = pyodbc.connect(connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
    "cursor = conn.cursor()\n",
    "\n",
    "if id is None:\n",
    "    query = f\"SELECT * FROM Metadata WHERE STATUS = 'active' ORDER BY ID DESC\"\n",
    "else:\n",
    "    query = f\"SELECT * FROM Metadata WHERE STATUS = 'active' AND ID = {id}\"\n",
    "\n",
    "\n",
    "# Test the connection\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "column_names = [column[0] for column in cursor.description]\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "df = pd.DataFrame.from_records(rows, columns=column_names)\n",
    "\n",
    "if df[\"ID\"].any():\n",
    "    pass\n",
    "else:\n",
    "    print(\"Nothing active\")\n",
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4fdae-35c3-49f5-900f-e5dd77892515",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def run_query_(query, snowflakeaccount, snowflakeuser, snowflakepassword):\n",
    "    with snowflake.connector.connect(account=snowflakeaccount,\n",
    "                                   user=snowflakeuser,\n",
    "                                   password=snowflakepassword) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            results = cur.execute(query).fetchall()\n",
    "    return results\n",
    "\n",
    "def get_service_client_token_credential_(credential) -> DataLakeServiceClient:\n",
    "    account_url = f\"https://onelake.dfs.fabric.microsoft.com/\"\n",
    "\n",
    "    service_client = DataLakeServiceClient(account_url, credential=credential)\n",
    "\n",
    "    return service_client\n",
    "\n",
    "\n",
    "def list_directory_contents(file_system_client: FileSystemClient, directory_name: str):\n",
    "    paths = file_system_client.get_paths(path=directory_name)\n",
    "\n",
    "    return paths\n",
    "# %%\n",
    "def upload_file_to_directory(directory_client: DataLakeDirectoryClient, local_path: str, file_name: str):\n",
    "    file_client = directory_client.get_file_client(file_name)\n",
    "\n",
    "    with open(file=os.path.join(local_path, file_name), mode=\"rb\") as data:\n",
    "        file_client.upload_data(data, overwrite=True)\n",
    "\n",
    "def get_secret_(secret_name, keyvault_name):\n",
    "    vaultBaseUrl = f\"https://{keyvault_name}.vault.azure.net\"\n",
    "    keyvault_token = local_credentials.get_token(vaultBaseUrl).token\n",
    "    \n",
    "    request_headers = {\n",
    "        \"Authorization\": f\"Bearer {keyvault_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    keyvault_url = f\"{vaultBaseUrl}/secrets/{secret_name}?api-version=7.4\"\n",
    "    response = requests.get(keyvault_url, headers=request_headers)\n",
    "    if response.status_code == 200:\n",
    "        secret_value = response.json()[\"value\"]\n",
    "        return secret_value\n",
    "    else:\n",
    "        raise Exception(f\"Failed to retrieve secret: {response.status_code} - {response.text}\")\n",
    "\n",
    "def get_full_data_type_definition_(snowflakeaccount, snowflakeuser, snowflakepassword, database, schema, table):\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        COLUMN_NAME,\n",
    "        CASE\n",
    "            WHEN DATA_TYPE IN ('NUMBER', 'DECIMAL', 'NUMERIC') AND NUMERIC_SCALE IS NOT NULL \n",
    "                THEN DATA_TYPE || '(' || NUMERIC_PRECISION || ',' || NUMERIC_SCALE || ')'\n",
    "            WHEN DATA_TYPE IN ('NUMBER', 'DECIMAL', 'NUMERIC') AND NUMERIC_SCALE IS NULL \n",
    "                THEN DATA_TYPE || '(' || NUMERIC_PRECISION || ')'\n",
    "            WHEN DATA_TYPE IN ('VARCHAR', 'CHAR', 'CHARACTER', 'STRING', 'TEXT') AND CHARACTER_MAXIMUM_LENGTH IS NOT NULL \n",
    "                THEN DATA_TYPE || '(' || CHARACTER_MAXIMUM_LENGTH || ')'\n",
    "            ELSE DATA_TYPE\n",
    "        END AS FULL_DATA_TYPE\n",
    "    FROM \n",
    "        {database}.INFORMATION_SCHEMA.COLUMNS \n",
    "    WHERE \n",
    "        TABLE_SCHEMA = '{schema}'\n",
    "        AND TABLE_NAME = '{table.upper()}'\n",
    "    ORDER BY \n",
    "        ORDINAL_POSITION\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = run_query_(query, snowflakeaccount, snowflakeuser, snowflakepassword)\n",
    "\n",
    "    schema_list = []\n",
    "    new_columns = []\n",
    "    for col in columns:\n",
    "        if col[0] == \"HASH_\":\n",
    "            continue\n",
    "        if \"NUMBER\" in col[1]:\n",
    "\n",
    "            decimal_1, decimal_2 = col[1].split(\"(\")[1].split(\",\")\n",
    "            decimal_2 = int(decimal_2.replace(\")\", \"\"))\n",
    "            decimal_1 = int(decimal_1)\n",
    "            \n",
    "            if int(decimal_2) > 0:\n",
    "                data_type = pa.decimal128(decimal_1, decimal_2)\n",
    "                col_d_type = \"decimal\"\n",
    "            else:\n",
    "                data_type = pa.int64()\n",
    "                col_d_type = \"int64\"\n",
    "\n",
    "        elif \"TIMESTAMP\" in col[1] or \"DATE\" in col[1]:\n",
    "            data_type = pa.timestamp('ns')\n",
    "            col_d_type = \"datetime64[ns]\"\n",
    "        elif \"BINARY\" in col[1]:\n",
    "            data_type = pa.binary()\n",
    "            col_d_type = \"object\"\n",
    "        elif \"BOOLEAN\" in col[1]:\n",
    "            data_type = pa.bool_()\n",
    "            col_d_type = \"bool\"\n",
    "        elif \"TIME\" in col[1]:\n",
    "            data_type = pa.time64('ns')\n",
    "            col_d_type = \"datetime64[ns]\"\n",
    "        elif \"DATE\" in col[1]:\n",
    "            data_type = pa.date32()\n",
    "            col_d_type = \"datetime64[ns]\"\n",
    "        else:\n",
    "            data_type = pa.string()\n",
    "            col_d_type = \"object\"\n",
    "\n",
    "        new_columns.append((col[0], col_d_type))\n",
    "        \n",
    "        schema_list.append((col[0], data_type))\n",
    "\n",
    "    schema_list.append((\"METADATAROW_ID\", pa.string()))\n",
    "    schema_list.append((\"__rowMarker__\", pa.int64()))\n",
    "\n",
    "    schema = pa.schema(schema_list)\n",
    "\n",
    "    return schema, columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37012df7-9a01-47f1-83f0-b25c89a54a01",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def run_for_one_table(df, local):\n",
    "\n",
    "    workspace_id = df[\"workspace_id\"]\n",
    "    mirrored_db_id = df[\"mirrored_db_id\"]\n",
    "\n",
    "    snowflake_db = df[\"snowflake_db\"]\n",
    "    snowflake_schema = df[\"snowflake_schema\"]\n",
    "    view_name = df[\"view_name\"]\n",
    "\n",
    "    streamable = df[\"streamable\"]\n",
    "\n",
    "    def get_secret(secret_name):\n",
    "        return get_secret_(secret_name, df[\"keyvault\"])\n",
    "\n",
    "    if local:\n",
    "\n",
    "        # Get the credentials by calling the API of the Key Vault 'https://{df[\"keyvault\"]}.vault.azure.net/'\n",
    "\n",
    "        mirroringspntenantid = get_secret('mirroringspntenantid')\n",
    "        mirroringspnclientid = get_secret('mirroringspnclientid')\n",
    "        mirroringspnclientsecret = get_secret('mirroringspnclientsecret')\n",
    "\n",
    "        snowflakeaccount = get_secret('snowflakeaccount')\n",
    "        snowflakeuser = get_secret('snowflakeuser')\n",
    "        snowflakepassword = get_secret('snowflakepassword')\n",
    "\n",
    "    else:\n",
    "        mirroringspntenantid = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'mirroringspntenantid')\n",
    "        mirroringspnclientid = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'mirroringspnclientid')\n",
    "        mirroringspnclientsecret = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'mirroringspnclientsecret')\n",
    "\n",
    "        snowflakeaccount = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'snowflakeaccount')\n",
    "        snowflakeuser = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'snowflakeuser')\n",
    "        snowflakepassword = notebookutils.credentials.getSecret(f'https://{df[\"keyvault\"]}.vault.azure.net/', 'snowflakepassword')\n",
    "\n",
    "\n",
    "    def run_query(query):\n",
    "        return run_query_(query, snowflakeaccount, snowflakeuser, snowflakepassword)\n",
    "    \n",
    "    def get_full_data_type_definition(view_name):\n",
    "        return get_full_data_type_definition_(snowflakeaccount, snowflakeuser, snowflakepassword, snowflake_db, snowflake_schema, view_name)\n",
    "\n",
    "    \n",
    "\n",
    "    credential = ClientSecretCredential(tenant_id=str(mirroringspntenantid),\n",
    "                        client_id=mirroringspnclientid,\n",
    "                        client_secret=mirroringspnclientsecret)\n",
    "\n",
    "    mirrored_db_path = f\"{mirrored_db_id}/Files/LandingZone\"\n",
    "\n",
    "    dlsc = get_service_client_token_credential_(credential)\n",
    "    fsc = dlsc.get_file_system_client(workspace_id)\n",
    "    table_path = mirrored_db_path + f\"/{view_name}\"\n",
    "    contents = list_directory_contents(fsc, directory_name=table_path)\n",
    "    content_names = [content[\"name\"] for content in contents]\n",
    "    partitionnames = [int(cont.split(\"/\")[-1].split(\".\")[0]) for cont in content_names if cont.endswith(\".parquet\")]\n",
    "    if len(partitionnames) < 1:\n",
    "        latest_partition = 0\n",
    "    else:\n",
    "        latest_partition = max(partitionnames)\n",
    "    latest_partition = str(latest_partition + 1)\n",
    "    while len(latest_partition) < 20:\n",
    "        latest_partition = \"0\" + latest_partition\n",
    "    latest_partition = latest_partition + \".parquet\"\n",
    "\n",
    "    if streamable == \"False\":\n",
    "\n",
    "        def df_to_delete_routine(df_to_delete, columns):\n",
    "            df_to_delete[\"__rowMarker__\"] = 2\n",
    "\n",
    "            for col in columns:\n",
    "                if col[0] == \"HASH_\":\n",
    "                    continue\n",
    "                if \"NUMBER\" in col[1] or \"FLOAT\" in col[1]:\n",
    "                    df_to_delete[col[0]] = 123\n",
    "                elif col[1] == \"BOOLEAN\":\n",
    "                    df_to_delete[col[0]] = True\n",
    "                elif \"DATE\" in col[1] or \"TIMESTAMP\" in col[1]:\n",
    "                    df_to_delete[col[0]] = \"2023-10-01 12:00:00.000\"\n",
    "                    df_to_delete[col[0]] = pd.to_datetime(df_to_delete[col[0]])\n",
    "                else:\n",
    "                    df_to_delete[col[0]] = \"Placeholder\"\n",
    "\n",
    "            df_to_delete[\"METADATAROW_ID\"] = df_to_delete[\"METADATAROW_ID\"].astype(str)\n",
    "\n",
    "            return df_to_delete\n",
    "        \n",
    "        def df_inserts_routine(df_inserts):\n",
    "            #df_inserts.rename(columns={col[0]: col[0].replace(\"_\",\"\").lower() for col in columns}, inplace=True)\n",
    "            df_inserts.rename(columns={\"HASH_\": \"METADATAROW_ID\"}, inplace=True)\n",
    "            df_inserts[\"__rowMarker__\"] = 0\n",
    "            df_inserts[\"METADATAROW_ID\"] = df_inserts[\"METADATAROW_ID\"].astype(str)\n",
    "            return df_inserts\n",
    "    \n",
    "        def query_routine(snowflake_db, snowflake_schema, view_name, batch_size, columns):\n",
    "            column_names = [col[0] for col in columns]\n",
    "            \n",
    "            column_names_str = \",\".join(column_names)\n",
    "            query = f\"SELECT {column_names_str} FROM {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_NEW LIMIT {batch_size};\"\n",
    "            results = run_query(query)\n",
    "\n",
    "            #df_inserts = create_typed_dataframe(results, columns)\n",
    "            df_inserts = pd.DataFrame(results, columns=column_names, dtype=\"object\")\n",
    "\n",
    "            rest = batch_size - df_inserts[column_names[0]].count() \n",
    "\n",
    "            if rest > 0:\n",
    "                query = f\"SELECT HASH_ FROM {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_OLD LIMIT {rest};\"\n",
    "                results = run_query(query)\n",
    "                df_to_delete = pd.DataFrame(results, columns=[\"METADATAROW_ID\"])\n",
    "            else:\n",
    "                df_to_delete = None\n",
    "            inserts = False\n",
    "            if int(df_inserts[\"HASH_\"].count()) > 0:\n",
    "                inserts = True\n",
    "                df_inserts = df_inserts_routine(df_inserts)\n",
    "\n",
    "            deletes = False\n",
    "\n",
    "            if df_to_delete is not None and int(df_to_delete[\"METADATAROW_ID\"].count()) > 0:\n",
    "                deletes = True\n",
    "                df_to_delete = df_to_delete_routine(df_to_delete, columns)\n",
    "            print(f\"Inserts: {inserts}, Deletes: {deletes}\")\n",
    "            if inserts:\n",
    "                print(f\"Insert count: {df_inserts['METADATAROW_ID'].count()}\")\n",
    "            if deletes:\n",
    "                print(f\"Delete count: {df_to_delete['METADATAROW_ID'].count()}\")        \n",
    "            \n",
    "            return inserts, df_inserts, deletes, df_to_delete\n",
    "\n",
    "\n",
    "        batch_size = 100000\n",
    "        \n",
    "        schema, columns = get_full_data_type_definition(f\"{view_name.upper()}_MIRROR_NEW\")\n",
    "\n",
    "        inserts, df_inserts, deletes, df_to_delete = query_routine(snowflake_db, snowflake_schema, view_name,\n",
    "                                                                   batch_size, columns)\n",
    "        \n",
    "        while inserts or deletes:\n",
    "\n",
    "            if inserts and deletes:\n",
    "                df_concat = pd.concat([df_inserts, df_to_delete], ignore_index=True, sort=False)\n",
    "            elif inserts and not deletes:\n",
    "                df_concat = df_inserts\n",
    "            elif deletes and not inserts:\n",
    "                df_concat = df_to_delete\n",
    "            else:\n",
    "                df_concat = None\n",
    "            os.makedirs('upload', exist_ok=True)\n",
    "\n",
    "            table = pa.Table.from_pandas(df_concat, schema=schema)\n",
    "            pq.write_table(table, f\"upload/{latest_partition}\")\n",
    "            \n",
    "            dldc = fsc.get_directory_client(table_path)\n",
    "            upload_file_to_directory(dldc, local_path=\"upload\", file_name=latest_partition)\n",
    "            count = df_concat[\"METADATAROW_ID\"].count()\n",
    "            print(f\"Uploaded {latest_partition} with row count {count}\")\n",
    "\n",
    "            if inserts:\n",
    "                hash_value_str = \",\".join([f\"('{hash_}')\" for hash_ in list(df_inserts[\"METADATAROW_ID\"])])\n",
    "                query = f\"\"\"INSERT INTO {snowflake_db}.{snowflake_schema}.{view_name.upper()}_MIRROR_HASHES (hash_) VALUES {hash_value_str}\"\"\"\n",
    "                run_query(query)\n",
    "            if deletes:\n",
    "                hash_value_str = \",\".join([f\"('{hash_}')\" for hash_ in list(df_to_delete[\"METADATAROW_ID\"])])\n",
    "                hash_value_str\n",
    "                query = f\"\"\"DELETE FROM {snowflake_db}.{snowflake_schema}.{view_name}_MIRROR_HASHES WHERE HASH_ in ({hash_value_str})\"\"\"\n",
    "                run_query(query)\n",
    "\n",
    "            latest_partition = str(int(latest_partition.split(\".\")[0]) + 1)\n",
    "            while len(latest_partition) < 20:\n",
    "                latest_partition = \"0\" + latest_partition\n",
    "            latest_partition = latest_partition + \".parquet\"\n",
    "\n",
    "            inserts, df_inserts, deletes, df_to_delete = query_routine(snowflake_db, snowflake_schema, view_name, batch_size, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05c47e-e8ae-4567-a5b9-af14fe1ad19f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(row[\"view_name\"])\n",
    "    run_for_one_table(row, local)\n",
    "    print(f\"Finished {row['view_name']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "c494b827-ca90-4f17-a799-e00cc98581de",
    "workspaceId": "5bddda80-bbab-40cf-b106-abb535fae28c"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
